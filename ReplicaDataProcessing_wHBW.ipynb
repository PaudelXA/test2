{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USAP093587\\AppData\\Local\\Temp\\ipykernel_18564\\3536400706.py:11: DtypeWarning: Columns (16,17,39,40,41,48,49,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df= pd.read_csv(r\"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset/Final/ReplicaOriginalDownload/replica-soundtransit_spring2023_weekday-02_22_24-trips_dataset.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of original download file is df- (8510893, 52)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#Read the csv from Original Downloads. Note: Replica website offers much more info (columns) than here. To reduce the file size, a subset of columns were only downloaded\n",
    "# For next time maybe look into for each file iterating over the folder #equivalent to batch scripting on Python                                                                                                        1\n",
    "#Spring2023\n",
    "#(Had to download Weekend data on 3-15-2024 because I did not include \"previous trip purpose\" in 02-22-2024 download)\n",
    "#file_path = \"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset/Final/ReplicaOriginalDownload/replica-soundtransit_spring2023_weekends-03_15_24-trips_dataset.csv\"\n",
    "#df= pd.read_csv(r\"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset/Final/ReplicaOriginalDownload/replica-soundtransit_spring2023_weekends-03_15_24-trips_dataset.csv\")\n",
    "file_path = \"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset/Final/ReplicaOriginalDownload/replica-soundtransit_spring2023_weekday-02_22_24-trips_dataset.csv\"\n",
    "df= pd.read_csv(r\"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset/Final/ReplicaOriginalDownload/replica-soundtransit_spring2023_weekday-02_22_24-trips_dataset.csv\")\n",
    "\n",
    "#sample = df.sample(1000)\n",
    "#taking a sample\n",
    "#df = df_Org.sample(1000)\n",
    "\n",
    "#Fall2022\n",
    "#file_path = \"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/DownloadsV3/replica-soundtransit_fall2022_weekend-02_22_24-trips_dataset.csv\"\n",
    "#df= pd.read_csv(r\"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/DownloadsV3/replica-soundtransit_fall2022_weekend-02_22_24-trips_dataset.csv\")\n",
    "#file_path = \"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/DownloadsV3/replica-soundtransit_fall2022_weekday-02_22_24-trips_dataset.csv\"\n",
    "#df= pd.read_csv(r\"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/DownloadsV3/replica-soundtransit_fall2022_weekday-02_22_24-trips_dataset.csv\")           #takes ~1min\n",
    "\n",
    "print(\"The shape of original download file is df-\", df.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a subset of the columns that we need\n",
    "subset_columns= ['origin_trct_2020', 'origin_trct_fips_2020', 'origin_cty_fips_2020', 'origin_cty_2020',                                      \n",
    "                 'destination_trct_2020', 'destination_trct_fips_2020', 'destination_cty_fips_2020', 'destination_cty_fips_2020',\n",
    "                 'primary_mode', 'trip_purpose', 'previous_trip_purpose',                                                                      #previous trip purpose\n",
    "                 'trip_start_time',  'trip_distance_miles', 'trip_duration_minutes', \n",
    "                 'trip_taker_household_income', \n",
    "                 'trip_taker_work_trct_2020', 'trip_taker_work_cty_2020','trip_taker_work_trct_fips_2020',\n",
    "                 'trip_taker_home_trct_2020', 'trip_taker_home_cty_2020',  'trip_taker_home_trct_fips_2020', \n",
    "                ]\n",
    "sub_df = df[subset_columns] \n",
    "print(\"After taking the immediate subset of columns, The shape of this file is\", sub_df.shape)\n",
    "\n",
    "## Filtering the counties\n",
    "filtered_df = (sub_df[(sub_df['trip_taker_work_cty_2020'].isin(['Pierce', 'King', 'Snohomish','Kitsap'])) &                                          # Update this on .py\n",
    "                      (sub_df['trip_taker_home_cty_2020'].isin(['Pierce', 'King', 'Snohomish','Kitsap']))])\n",
    "print(\"After filtering trip taker's work home & office within Pierce, King, Snohomish, Kitsap, shape is\",filtered_df.shape)\n",
    "\n",
    "# Checking\n",
    "print(\"Trip taker's work\", filtered_df['trip_taker_work_cty_2020'].value_counts(), \"& empty rows are\",filtered_df['trip_taker_work_cty_2020'].isna().sum() )\n",
    "print(\"Trip taker's home\",filtered_df['trip_taker_home_cty_2020'].value_counts(), \"& empty rows are\",filtered_df['trip_taker_home_cty_2020'].isna().sum() )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rounding time to the nearest Hour\n",
    "filtered_df['trip_start_time'] = pd.to_datetime(filtered_df['trip_start_time']).dt.round('H').dt.hour\n",
    "\n",
    "import numpy as np\n",
    "#Rounding the trip distance\n",
    "filtered_df['trip_distance_miles'] = filtered_df['trip_distance_miles'].apply(np.ceil)\n",
    "\n",
    "print(\"Rows filtered for Orign, Destination in Replica & trip_taker's home and office here in this code\", filtered_df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_df[ 'trip_taker_work_trct_fips_2020'].shape\n",
    "#df[ 'trip_taker_work_trct_fips_2020'].unique\n",
    "#filtered_df.loc[filtered_df['trip_taker_work_trct_fips_2020'] == \"Does not have work/school location\"]      #1_718_423 from df #0 from filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some trips does not have Trip taker's work and/or home location information. For these ones, FIPS tract values are string. \n",
    "# Now deleting all non-numeric rows from trip_taker_work_trct_fips_2020 & other as necessary\n",
    "# Big Learning here .loc doesnt change the data type\n",
    "filtered_df[ 'trip_taker_work_trct_fips_2020'] = pd.to_numeric(filtered_df['trip_taker_work_trct_fips_2020'], errors='coerce').astype('Int64')\n",
    "filtered_df['trip_taker_home_trct_fips_2020'] = pd.to_numeric(filtered_df['trip_taker_home_trct_fips_2020'], errors='coerce').astype('Int64')\n",
    "filtered_df['origin_trct_fips_2020'] = pd.to_numeric(filtered_df['origin_trct_fips_2020'], errors='coerce').astype('Int64')\n",
    "\n",
    "print(\"Trip taker's home\",filtered_df['trip_taker_home_trct_fips_2020'].value_counts(), \"& empty rows are\",filtered_df['trip_taker_home_trct_fips_2020'].isna().sum() )\n",
    "print(\"Trip taker's work\",filtered_df['trip_taker_work_trct_fips_2020'].value_counts(), \"& empty rows are\",filtered_df['trip_taker_work_trct_fips_2020'].isna().sum() )\n",
    "print(\"Original code dropped all empty values here\")\n",
    "# Should I drop all \"Nan\" rows? Maybe at the end\n",
    "#filtered_df.dropna(subset=['trip_taker_work_trct_fips_2020'], inplace=True)                                                 #didnt do much for spring23 weekday Original code drops this\n",
    "\n",
    "print(\"The shape is now\", filtered_df.shape)                   #from ReplicaOrg= (7885167, 53)>> (5167903, 20) >> (5167903, 20)\n",
    "filtered_df[\"trip_taker_home_trct_fips_2020\"].value_counts()\n",
    "print(\"Trip taker tract has\", filtered_df[\"trip_taker_home_trct_fips_2020\"].isna().sum(), \"empty values\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if any empty values in 'trip_taker_household_income_bin\n",
    "#print(\"The empty values in Trip taker HH income is \",filtered_df[\"trip_taker_household_income\"].isnull().sum())  \n",
    "\n",
    "# Converting income to bins\n",
    "# 1 -- Less than or equal to 25000\n",
    "# 2 -- More than 25000 and less than or equal to 50000\n",
    "# 3 -- More than 50000 and less than or equal to 75000\n",
    "# 4 -- More than 75000 and less than or equal to 100000\n",
    "# 5 --More than 100000\n",
    "# Define the bin edges and labels\n",
    "bin_edges = [-float('inf'), 0, 25000, 50000, 75000, 100000, float('inf')]        #Note: There are some -ve income which is converted to Bin1\n",
    "bin_labels = [1, 1, 2, 3, 4, 5]\n",
    "# Use pd.cut to categorize the column values\n",
    "filtered_df.loc[:,'trip_taker_household_income_bin'] = pd.cut(filtered_df['trip_taker_household_income'], bins=bin_edges, labels=bin_labels, right=False, ordered=False)#.fillna('9)')                            #change in .py NA-changes\n",
    "\n",
    "\n",
    "# Checking\n",
    "#filtered_df[\"trip_taker_household_income\"].describe()        #has negative values too\n",
    "#filtered_df[\"trip_taker_household_income\"].info()            #Confirmed 0 empty\n",
    "#df=filtered_df\n",
    "#df[df[\"trip_taker_household_income_bin\"].isnull()][['trip_taker_household_income_bin', 'trip_taker_household_income']].info()\n",
    "#df[df[\"trip_taker_household_income_bin\"].isnull()][['trip_taker_household_income_bin', 'trip_taker_household_income']].info()   664 rows\n",
    "\n",
    "#filtered_df[\"trip_taker_household_income_bin\"].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Subset_for_mapping = filtered_df.copy()\n",
    "# To Map tracts to Travelshed                                                                                                                                                                                    2 Dict Vs Join\n",
    "# Get the mapping file\n",
    "TravelShed_w_all_FIPS = pd.read_csv(f\"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/TravelShed_FIPS.csv\")\n",
    "# This File is located in Sharepoint with block group code(bg_code) and its corresponding travelshed. Took out the last digit to get Tract FIPS and use it to map.   \n",
    "\n",
    "# Put csv in the dictionary\n",
    "TravelShed_dict = dict(zip(TravelShed_w_all_FIPS['tract'], TravelShed_w_all_FIPS['travelshed']))\n",
    "\n",
    "#Mapping tract from original dataframe to Travelshed to new columns\n",
    "Subset_for_mapping['Origin_sline_shed'] = Subset_for_mapping['origin_trct_fips_2020'].map(TravelShed_dict).fillna(99)#, inplace=True) \n",
    "Subset_for_mapping['Destination_sline_shed'] = Subset_for_mapping['destination_trct_fips_2020'].map(TravelShed_dict).fillna(99)#, inplace=True)\n",
    "Subset_for_mapping['Trip_taker_home_sline_shed'] = Subset_for_mapping['trip_taker_home_trct_fips_2020'].map(TravelShed_dict).fillna(99)#, inplace=True)\n",
    "Subset_for_mapping['Trip_taker_work_sline_shed'] = Subset_for_mapping['trip_taker_work_trct_fips_2020'].map(TravelShed_dict).fillna(99)#,inplace=True)\n",
    "\n",
    "#Checking if location and TravelShed mapping went OK\n",
    "#Subset_for_mapping[['Origin_sline_shed' ,'origin_trct_fips_2020', 'Destination_sline_shed' ,'destination_trct_fips_2020']].head(10)\n",
    "#Subset_for_mapping[['Trip_taker_home_sline_shed' ,'trip_taker_home_trct_fips_2020', 'trip_taker_home_trct_2020', 'Trip_taker_work_sline_shed' ,'trip_taker_work_trct_fips_2020', 'trip_taker_work_trct_2020']].value_counts()\n",
    "\n",
    "print(\"The shape after Travel Sheds is\",  Subset_for_mapping.shape)           # 4 travel sheds: 'Origin_sline_shed', 'Destination_sline_shed','Trip_taker_home_sline_shed', 'Trip_taker_work_sline_shed'\n",
    "                                                                              #+ 1 Income bin: 'trip_taker_household_income_bin'\n",
    "\n",
    "\n",
    "print(\"The trip taker's home Travel-Shed are\",Subset_for_mapping[\"Trip_taker_home_sline_shed\"].value_counts())\n",
    "print(\"The trip taker's home Travel-Shed are\",Subset_for_mapping[\"Trip_taker_work_sline_shed\"].value_counts())\n",
    "print(\"The trip Origin Travel-Shed are\",Subset_for_mapping[\"Origin_sline_shed\"].value_counts())\n",
    "print(\"The trip Destination Travel-Shed are\",Subset_for_mapping[\"Destination_sline_shed\"].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset_for_mapping.columns\n",
    "#Subset_for_mapping.head(10)\n",
    "\n",
    "subset_for_summary =Subset_for_mapping.copy()\n",
    "\n",
    "# Start time >> Start hour\n",
    "#subset_for_summary.rename(columns = {'trip_start_time':'trip_start_hour'}, inplace = True) \n",
    "# drop county code, trip duration, others\n",
    "#subset_for_summary = subset_for_summary.drop(['origin_cty_fips_2020','origin_trct_2020','destination_trct_2020', 'destination_cty_fips_2020','trip_taker_home_trct_2020','trip_taker_home_cty_2020', 'trip_duration_minutes'],axis=1) #,'trip_duration_minutes' #'origin_cty_fips_2020','trip_taker_work_trct_2020.1',\n",
    "print(\"The shape is now\", subset_for_summary.shape)\n",
    "print(\"The columns remaining are\", subset_for_summary.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now categorizing these columns to numericals\n",
    "#Primary-mode ['private_auto', 'auto_passenger', 'public_transit','other_travel_mode', 'on_demand_auto', 'biking', 'walking'], dtype=object)\n",
    "# 1 'private_auto'\n",
    "# 2  'auto_passenger'\n",
    "# 3 'public_transit'\n",
    "# 4 'on_demand_auto'\n",
    "# 5 'biking'\n",
    "# 6 'walking'\n",
    "# 7 'other_travel_mode'\n",
    "\n",
    "#Trip purpose 'home', 'work', 'shop', 'social', 'eat', 'other_activity_type', 'maintenance', 'school', 'recreation'\n",
    "#1 'home'\n",
    "#2 'work'\n",
    "#3 'shop'\n",
    "#4 'social'\n",
    "#5 'eat'\n",
    "#6 'school'\n",
    "#7 'recreation'\n",
    "#8 'maintenance'\n",
    "#9 'other_activity_type'\n",
    "\n",
    "#TravelShed\n",
    "\n",
    "#Origin_sline_shed =           ['99', 'Tukwila', 'Auburn', 'Kent', 'Sumner', 'Lakewood', 'Puyallup', 'King Street', 'Tacoma Dome', 'South Tacoma'],dtype=object)\n",
    "#'Destination_sline_shed'=     ['99', 'Kent', 'King Street', 'Tukwila', 'Puyallup', 'Sumner',  'Tacoma Dome', 'Auburn', 'Lakewood', 'South Tacoma'], dtype=object)\n",
    "#'Trip_taker_home_sline_shed'= ['99', 'Kent', 'Tacoma Dome', 'Sumner', 'Lakewood', 'Puyallup','Auburn', 'King Street', 'Tukwila', 'South Tacoma'], dtype=object)\n",
    "#'Trip_taker_work_sline_shed'= ['99', 'King Street', 'Tukwila', 'Kent', 'Auburn', 'Tacoma Dome', 'Puyallup', 'Sumner', 'Lakewood', 'South Tacoma'], dtype=object) \n",
    "#df['trip_purpose'].unique() \n",
    "\n",
    "# Categorizing a few columns to reduce the file size\n",
    "# Creating an empty Dictionary\n",
    "Dict = {}\n",
    "Shed_list = [('Auburn',1), ('Kent',2), ('King Street',3), ('Lakewood',4), ('Puyallup',5), ('South Tacoma',6), ('Sumner',7), ('Tacoma Dome',8), ('Tukwila',9), (99,99)]\n",
    "Mode_list = [('private_auto',1), ('auto_passenger',2), ('public_transit',3), ('on_demand_auto',4), ('biking',5), ('walking',6),('other_travel_mode',7)]\n",
    "Trp_purpose_list= [('home',1), ('work',2), ('shop',3), ('social',4), ('eat',5), ('school',6), ('recreation',7), ('maintenance',8), ('other_activity_type',9)]\n",
    " \n",
    "# Creating a Dictionary # with each item as a Pair\n",
    "Shed_Dict = dict(Shed_list)\n",
    "Mode_Dict = dict(Mode_list)\n",
    "Trp_purpose_Dict= dict(Trp_purpose_list)\n",
    "#print(Dict)\n",
    "\n",
    "\n",
    "#Now Mapping\n",
    "subset_for_summary['Origin_sline_shed_numeric'] = subset_for_summary['Origin_sline_shed'].map(Shed_Dict) \n",
    "subset_for_summary['Destination_sline_shed_numeric'] = subset_for_summary['Destination_sline_shed'].map(Shed_Dict) \n",
    "subset_for_summary['Trip_taker_home_sline_shed_numeric'] = subset_for_summary['Trip_taker_home_sline_shed'].map(Shed_Dict) \n",
    "subset_for_summary['Trip_taker_work_sline_shed_numeric'] = subset_for_summary['Trip_taker_work_sline_shed'].map(Shed_Dict) \n",
    "\n",
    "subset_for_summary['Primary_Mode_numeric'] = subset_for_summary['primary_mode'].map(Mode_Dict) \n",
    "subset_for_summary['Trip_purpose_numeric'] = subset_for_summary['trip_purpose'].map(Trp_purpose_Dict)\n",
    "\n",
    "# checking the map \n",
    "subset_for_summary [['Origin_sline_shed','Origin_sline_shed_numeric','Destination_sline_shed', 'Destination_sline_shed_numeric','Trip_taker_home_sline_shed','Trip_taker_home_sline_shed_numeric','Trip_taker_work_sline_shed','Trip_taker_work_sline_shed_numeric','primary_mode','Primary_Mode_numeric','trip_purpose','Trip_purpose_numeric']]\n",
    "\n",
    "subset_for_summary[['Origin_sline_shed','Origin_sline_shed_numeric','Destination_sline_shed', 'Destination_sline_shed_numeric','Trip_taker_home_sline_shed','Trip_taker_home_sline_shed_numeric','Trip_taker_work_sline_shed','Trip_taker_work_sline_shed_numeric','primary_mode','Primary_Mode_numeric','trip_purpose','Trip_purpose_numeric']].tail(10)\n",
    "print(subset_for_summary[\"Origin_sline_shed\"].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding HBW trips only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, trying to get only HBW trips & its hourly counts\n",
    "#For HBW Columns\n",
    "HBW_col = ['trip_purpose','previous_trip_purpose' ]\n",
    "\n",
    "# Concatenate values of all columns to create a new column 'concatenated'\n",
    "subset_for_summary['HBW_concat'] = subset_for_summary[HBW_col].agg('-'.join, axis=1)\n",
    "\n",
    "#subset_for_summary[['trip_purpose','previous_trip_purpose','HBW_concat','trip_start_time']]\n",
    "subset_for_summary =  subset_for_summary[subset_for_summary['HBW_concat'].str.contains('work-home|home-work')] \n",
    "print(\"The HBW trips table's size is\", subset_for_summary.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediary file writeup, before making trip counts for each matching rows\n",
    "file_name = os.path.basename(file_path).replace(\".csv\", \"_afterNumeric.csv\")\n",
    "file_path = os.path.join(\"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset/Final\", file_name).replace(\"\\\\\",\"/\")\n",
    "subset_for_summary.to_csv(file_path, index=False)\n",
    "\n",
    "#Since mapping is ok\n",
    "drop_col = ['trip_taker_work_cty_2020','Origin_sline_shed','Destination_sline_shed', 'Trip_taker_home_sline_shed','Trip_taker_work_sline_shed','primary_mode', 'trip_purpose']\n",
    "subset_after_numeric = subset_for_summary.drop(columns=drop_col, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_after_numeric = subset_for_summary    #Comment it later\n",
    "#Lets rename the numeric columns & others\n",
    "subset_after_numeric = subset_after_numeric.rename(columns={'Origin_sline_shed_numeric': 'origin_travel_shed', \n",
    "                        'Destination_sline_shed_numeric': 'destination_travel_shed',\n",
    "                        'Trip_taker_home_sline_shed_numeric':'trip_taker_home_travel_shed',\n",
    "                        'Trip_taker_work_sline_shed_numeric': 'trip_taker_work_travel_shed',\n",
    "                        'Primary_Mode_numeric':'primary_mode',\n",
    "                        'Trip_purpose_numeric':'trip_purpose',\n",
    "                        'trip_start_time':'trip_start_hour'\n",
    "                        })\n",
    "\n",
    "print(\"Now the shape is\", subset_after_numeric.shape)\n",
    "print(\"The columns are\", subset_after_numeric.columns)\n",
    "\n",
    "final_subset = subset_after_numeric[['origin_trct_fips_2020','destination_trct_fips_2020','primary_mode', 'trip_purpose','previous_trip_purpose','trip_start_hour','trip_distance_miles','trip_taker_work_trct_fips_2020','trip_taker_home_trct_fips_2020','trip_taker_household_income_bin','origin_travel_shed',\n",
    "       'destination_travel_shed', 'trip_taker_home_travel_shed','trip_taker_work_travel_shed', 'HBW_concat' ]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the trip counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_subset\n",
    "# Concatenate values of all columns to create a new column 'concatenated'\n",
    "final_subset['concatenated'] = final_subset[final_subset.columns].astype(str).agg('-'.join, axis=1)\n",
    "\n",
    "# Create a new column 'counts' by counting occurrences of each unique combination\n",
    "final_subset['trip_counts'] = final_subset.groupby('concatenated')['concatenated'].transform('count')        # I could use trip counts\n",
    "\n",
    "# Drop the 'concatenated' column if not needed anymore\n",
    "final_subset = final_subset.drop(columns=['concatenated'])\n",
    "#final_subset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_subset.shape    #(1409494, 18)\n",
    "#final_subset['trip_taker_work_travel_shed'].value_counts()\n",
    "#843931/1_409_494\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hourly_counts=final_subset.groupby('trip_start_hour').size()\n",
    "\n",
    "#To minimize the file size # I should have done much earlier\n",
    "cols_toInt8 = ['primary_mode', 'trip_purpose','trip_start_hour','trip_distance_miles','origin_travel_shed','destination_travel_shed','trip_taker_home_travel_shed','trip_taker_work_travel_shed','trip_counts']\n",
    "final_subset[cols_toInt8] = final_subset[cols_toInt8].applymap(np.int8)\n",
    "\n",
    "# Writing final_subset\n",
    "#Subset_before_merged = \"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset\"\n",
    "#file_path = # already given above while reading original replica download\n",
    "file_name = os.path.basename(file_path).replace(\"_afterNumeric.csv\", \"_subset.csv\")\n",
    "file_path = os.path.join(\"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset/Final/Final_Subset\", file_name).replace(\"\\\\\",\"/\")\n",
    "final_subset.to_csv(file_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "#Sample\n",
    "file_name = os.path.basename(file_path).replace( \"_afterNumeric.csv\", \".csv\")\n",
    "path=os.path.join(\"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset/Final/Sample_final\",\"_FinalSubset_Sample.csv\").replace(\"\\\\\",\"/\")\n",
    "final_subset.sample(100).to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_subset['HBW_concat'].unique()\n",
    "final_subset.shape\n",
    "final_subset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking trips between travelsheds\n",
    "\n",
    "trips_within_travelshed = final_subset[(final_subset['origin_travel_shed'].isin([1,2,3,4,5,6,7,8,9,99])) & \n",
    "                                             (final_subset['destination_travel_shed'].isin([1,2,3,4,5,6,7,8,9]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trips_within_travelshed.shape\n",
    "#trips_within_travelshed['origin_travel_shed'].value_counts()\n",
    "#trips_within_travelshed['destination_travel_shed'].value_counts()\n",
    "#trips_within_travelshed['trip_taker_home_travel_shed'].value_counts()\n",
    "trips_within_travelshed['trip_taker_work_travel_shed'].value_counts()             #.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let me explore those external-to-Travel-Shed trips\n",
    "trips_within_travelshed['trip_taker_work_travel_shed']\n",
    "trips_originating_ExternalToTraveShed =  trips_within_travelshed[trips_within_travelshed['origin_travel_shed']==99] \n",
    "#trips_originating_ExternalToTraveShed.shape\n",
    "#trips_originating_ExternalToTraveShed\n",
    "\n",
    "#trips_originating_ExternalToTraveShed.groupby('origin_trct_fips_2020').size().sort_values(ascending=False) .head(10)    \n",
    "trips_originating_ExternalToTraveShed.groupby('destination_trct_fips_2020').size().sort_values(ascending=False) .head(10)   \n",
    "# Most trips seems to be coming from \n",
    "#origin_trct_fips_2020\n",
    "#               TRIPS\n",
    "# 53033022803    3546       Kings County\n",
    "# 53033010900    2756       Kings County\n",
    "# 53033028402    2411       Kings County\n",
    "# 53053070703    1743       Snohomish County\n",
    "# 53033027200    1737       Kings County\n",
    "\n",
    "\n",
    "#hourly_counts=trips_within_travelshed.groupby('trip_start_hour').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_within_travelshed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #For HBW Columns\n",
    "# HBW_col = ['trip_purpose','previous_trip_purpose' ]\n",
    "\n",
    "# #final_subset\n",
    "# # Concatenate values of all columns to create a new column 'concatenated'\n",
    "# subset_for_summary['HBW_concat'] = subset_for_summary[HBW_col].agg('-'.join, axis=1)\n",
    "\n",
    "# subset_for_summary[['trip_purpose','previous_trip_purpose','HBW_concat','trip_start_time']]\n",
    "# subset_for_summary =  subset_for_summary[subset_for_summary['HBW_concat'].str.contains('work-home|home-work')] \n",
    "# print(\"The HBW trips table's size is\", subset_for_summary.shape)\n",
    "\n",
    "hourly_counts=trips_within_travelshed.groupby('trip_start_hour').size()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a histogram\n",
    "plt.figure(figsize=(10, 6))  # Optional: Set the figure size\n",
    "plt.bar(hourly_counts.index, hourly_counts, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Hourly Count of Trips')\n",
    "plt.grid(axis='y')  # Optional: Add grid lines\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3_641_088/6269415 \n",
    "#+426_677+ 272_632+ 252_366+ 243_885+ 218_106+211_406+ 166_186+ 154_695+ 74_917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_subset['HBW_concat'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_for_summary_HBW[['trip_purpose','previous_trip_purpose','HBW_concat','trip_start_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_df_HBW =  filtered_df[filtered_df['HBW_concat'].str.contains('work-home|home-work')]  #1000 > (165, 23)\n",
    "\n",
    "subset_after_numeric.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_after_numeric[['trip_start_hour','trip_purpose','previous_trip_purpose']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_df['previous_trip_purpose'].unique() #['eat', 'maintenance', 'recreation', 'work_from_home', 'school','social', 'work', 'home', 'shop', 'other_activity_type']\n",
    "filtered_df['trip_purpose'].unique()           #[ 'eat', 'maintenance', 'recreation',                   'school','social', 'work', 'home', 'shop', 'other_activity_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset/Final/ReplicaOriginalDownload/replica-soundtransit_spring2023_weekday-03_14_24-trips_dataset.csv\"\n",
    "df_Org= pd.read_csv(r\"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset/Final/ReplicaOriginalDownload/replica-soundtransit_spring2023_weekday-03_14_24-trips_dataset.csv\")\n",
    "df_Org.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset/Final/ReplicaOriginalDownload/replica-soundtransit_spring2023_weekday-02_22_24-trips_dataset.csv\"\n",
    "df_Org_Feb= pd.read_csv(r\"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset/Final/ReplicaOriginalDownload/replica-soundtransit_spring2023_weekday-02_22_24-trips_dataset.csv\")\n",
    "df_Org_Feb.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code that takes subset of Replica data & filters HBW Trips only too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# from scratch with 2/24 datasets\n",
    "#Updated 3/15/2024\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#Read the csv from Original Downloads. Note: Replica website offers much more info (columns) than here. To reduce the file size, a subset of columns were only downloaded\n",
    "# For next time maybe look into for each file iterating over the folder #equivalent to batch scripting on Python                                                                                                        1\n",
    "#Spring2023\n",
    "file_path = \"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset/Final/ReplicaOriginalDownload/replica-soundtransit_spring2023_weekends-03_15_24-trips_dataset.csv\"\n",
    "df= pd.read_csv(r\"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset/Final/ReplicaOriginalDownload/replica-soundtransit_spring2023_weekends-03_15_24-trips_dataset.csv\")\n",
    "#file_path = \"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset/Final/ReplicaOriginalDownload/replica-soundtransit_spring2023_weekday-02_22_24-trips_dataset.csv\"\n",
    "#df= pd.read_csv(r\"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset/Final/ReplicaOriginalDownload/replica-soundtransit_spring2023_weekday-02_22_24-trips_dataset.csv\")\n",
    "\n",
    "#sample = df.sample(1000)\n",
    "#taking a sample\n",
    "#df = df_Org.sample(1000)\n",
    "\n",
    "#Fall2022\n",
    "#file_path = \"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/DownloadsV3/replica-soundtransit_fall2022_weekend-02_22_24-trips_dataset.csv\"\n",
    "#df= pd.read_csv(r\"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/DownloadsV3/replica-soundtransit_fall2022_weekend-02_22_24-trips_dataset.csv\")\n",
    "#file_path = \"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/DownloadsV3/replica-soundtransit_fall2022_weekday-02_22_24-trips_dataset.csv\"\n",
    "#df= pd.read_csv(r\"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/DownloadsV3/replica-soundtransit_fall2022_weekday-02_22_24-trips_dataset.csv\")           #takes ~1min\n",
    "\n",
    "print(\"The shape of this file is\", df.shape)\n",
    "\n",
    "# Taking a subset of the columns that we need\n",
    "\n",
    "subset_columns= ['origin_trct_2020', 'origin_trct_fips_2020', 'origin_cty_fips_2020', 'origin_cty_2020',                                      \n",
    "                 'destination_trct_2020', 'destination_trct_fips_2020', 'destination_cty_fips_2020', 'destination_cty_fips_2020',\n",
    "                 'primary_mode', 'trip_purpose', 'previous_trip_purpose',                                                                      #previous trip purpose\n",
    "                 'trip_start_time',  'trip_distance_miles', 'trip_duration_minutes', \n",
    "                 'trip_taker_household_income', \n",
    "                 'trip_taker_work_trct_2020', 'trip_taker_work_cty_2020','trip_taker_work_trct_fips_2020',\n",
    "                 'trip_taker_home_trct_2020', 'trip_taker_home_cty_2020',  'trip_taker_home_trct_fips_2020', \n",
    "                ]\n",
    "sub_df = df[subset_columns] \n",
    "print(\"After taking the immediate subset of columns, The shape of this file is\", sub_df.shape)\n",
    "\n",
    "## Filtering the counties\n",
    "#filtered_df = sub_df.loc[sub_df['trip_taker_work_cty_2020'].isin(['Pierce', 'King', 'Snohomish','Kitsap' ])]\n",
    "#print(\"After filtering trip taker's work city within Pierce, King, Snohomish, Kitsap, shape is\",filtered_df.shape)     #1000>>956\n",
    "\n",
    "#filtered_df = sub_df.loc[sub_df['trip_taker_home_cty_2020'].isin(['Pierce', 'King', 'Snohomish','Kitsap' ])]         \n",
    "#print(\"After filtering trip taker's work home within Pierce, King, Snohomish, Kitsap, shape is\",filtered_df.shape)\n",
    "\n",
    "filtered_df = (sub_df[(sub_df['trip_taker_work_cty_2020'].isin(['Pierce', 'King', 'Snohomish','Kitsap'])) &                                          # Update this on .py\n",
    "                      (sub_df['trip_taker_home_cty_2020'].isin(['Pierce', 'King', 'Snohomish','Kitsap']))])\n",
    "print(\"After filtering trip taker's work home & office within Pierce, King, Snohomish, Kitsap, shape is\",filtered_df.shape)\n",
    "print(\"Mind that trip data was downloaded where trips' origin & destination were in 5 neighbouring counties\")\n",
    "\n",
    "# Checking\n",
    "print(\"Trip taker's work\", filtered_df['trip_taker_work_cty_2020'].value_counts(), \"& empty rows are\",filtered_df['trip_taker_work_cty_2020'].isna().sum() )\n",
    "print(\"Trip taker's home\",filtered_df['trip_taker_home_cty_2020'].value_counts(), \"& empty rows are\",filtered_df['trip_taker_home_cty_2020'].isna().sum() )\n",
    "\n",
    "# Rounding time to the nearest Hour\n",
    "filtered_df['trip_start_time'] = pd.to_datetime(filtered_df['trip_start_time']).dt.round('H').dt.hour\n",
    "\n",
    "import numpy as np\n",
    "#Rounding the trip distance\n",
    "filtered_df['trip_distance_miles'] = filtered_df['trip_distance_miles'].apply(np.ceil)\n",
    "\n",
    "print(\"Rows filtered for Orign,Destination in Replica & trip_taker's home and office here in this code\", filtered_df.shape)\n",
    "\n",
    "# Some trips does not have Trip taker's work and/or home location information. For these ones, FIPS tract values are string. \n",
    "# Now deleting all non-numeric rows from trip_taker_work_trct_fips_2020 & other as necessary\n",
    "# Big Learning here .loc doesnt change the data type\n",
    "filtered_df[ 'trip_taker_work_trct_fips_2020'] = pd.to_numeric(filtered_df['trip_taker_work_trct_fips_2020'], errors='coerce').astype('Int64')\n",
    "filtered_df['trip_taker_home_trct_fips_2020'] = pd.to_numeric(filtered_df['trip_taker_home_trct_fips_2020'], errors='coerce').astype('Int64')\n",
    "filtered_df['origin_trct_fips_2020'] = pd.to_numeric(filtered_df['origin_trct_fips_2020'], errors='coerce').astype('Int64')\n",
    "\n",
    "print(\"Trip taker's home\",filtered_df['trip_taker_home_trct_fips_2020'].value_counts(), \"& empty rows are\",filtered_df['trip_taker_home_trct_fips_2020'].isna().sum() )\n",
    "print(\"Trip taker's work\",filtered_df['trip_taker_work_trct_fips_2020'].value_counts(), \"& empty rows are\",filtered_df['trip_taker_work_trct_fips_2020'].isna().sum() )\n",
    "print(\"Original code dropped all empty values here\")\n",
    "# Should I drop all \"Nan\" rows? Maybe at the end\n",
    "#filtered_df.dropna(subset=['trip_taker_work_trct_fips_2020'], inplace=True)                                                 #didnt do much for spring23 weekday Original code drops this\n",
    "\n",
    "print(\"The shape is now\", filtered_df.shape)                   #from ReplicaOrg= (7885167, 53)>> (5167903, 20) >> (5167903, 20)\n",
    "filtered_df[\"trip_taker_home_trct_fips_2020\"].value_counts()\n",
    "print(\"Trip taker tract has\", filtered_df[\"trip_taker_home_trct_fips_2020\"].isna().sum(), \"empty values\")\n",
    "\n",
    "#Checking if any empty values in 'trip_taker_household_income_bin\n",
    "#print(\"The empty values in Trip taker HH income is \",filtered_df[\"trip_taker_household_income\"].isnull().sum())  \n",
    "\n",
    "# Converting income to bins\n",
    "# 1 -- Less than or equal to 25000\n",
    "# 2 -- More than 25000 and less than or equal to 50000\n",
    "# 3 -- More than 50000 and less than or equal to 75000\n",
    "# 4 -- More than 75000 and less than or equal to 100000\n",
    "# 5 --More than 100000\n",
    "# Define the bin edges and labels\n",
    "bin_edges = [-float('inf'), 0, 25000, 50000, 75000, 100000, float('inf')]\n",
    "bin_labels = [1, 1, 2, 3, 4, 5]\n",
    "# Use pd.cut to categorize the column values\n",
    "filtered_df.loc[:,'trip_taker_household_income_bin'] = pd.cut(filtered_df['trip_taker_household_income'], bins=bin_edges, labels=bin_labels, right=False, ordered=False)#.fillna('9)')                            #change in .py NA-changes\n",
    "\n",
    "\n",
    "# Checking\n",
    "#filtered_df[\"trip_taker_household_income\"].describe()        #has negative values too\n",
    "#filtered_df[\"trip_taker_household_income\"].info()            #Confirmed 0 empty\n",
    "#df=filtered_df\n",
    "#df[df[\"trip_taker_household_income_bin\"].isnull()][['trip_taker_household_income_bin', 'trip_taker_household_income']].info()\n",
    "#df[df[\"trip_taker_household_income_bin\"].isnull()][['trip_taker_household_income_bin', 'trip_taker_household_income']].info()   664 rows\n",
    "\n",
    "filtered_df[\"trip_taker_household_income_bin\"].unique()\n",
    "\n",
    "Subset_for_mapping = filtered_df.copy()\n",
    "# To Map tracts to Travelshed                                                                                                                                                                                    2 Dict Vs Join\n",
    "# Get the mapping file\n",
    "TravelShed_w_all_FIPS = pd.read_csv(f\"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/TravelShed_FIPS.csv\")\n",
    "# This File is located in Sharepoint with block group code(bg_code) and its corresponding travelshed. Took out the last digit to get Tract FIPS and use it to map.   \n",
    "\n",
    "# Put csv in the dictionary\n",
    "TravelShed_dict = dict(zip(TravelShed_w_all_FIPS['tract'], TravelShed_w_all_FIPS['travelshed']))\n",
    "\n",
    "#Mapping tract from original dataframe to Travelshed to new columns\n",
    "Subset_for_mapping['Origin_sline_shed'] = Subset_for_mapping['origin_trct_fips_2020'].map(TravelShed_dict).fillna(99)#, inplace=True) \n",
    "Subset_for_mapping['Destination_sline_shed'] = Subset_for_mapping['destination_trct_fips_2020'].map(TravelShed_dict).fillna(99)#, inplace=True)\n",
    "Subset_for_mapping['Trip_taker_home_sline_shed'] = Subset_for_mapping['trip_taker_home_trct_fips_2020'].map(TravelShed_dict).fillna(99)#, inplace=True)\n",
    "Subset_for_mapping['Trip_taker_work_sline_shed'] = Subset_for_mapping['trip_taker_work_trct_fips_2020'].map(TravelShed_dict).fillna(99)#,inplace=True)\n",
    "\n",
    "#Checking if location and TravelShed mapping went OK\n",
    "#Subset_for_mapping[['Origin_sline_shed' ,'origin_trct_fips_2020', 'Destination_sline_shed' ,'destination_trct_fips_2020']].head(10)\n",
    "#Subset_for_mapping[['Trip_taker_home_sline_shed' ,'trip_taker_home_trct_fips_2020', 'trip_taker_home_trct_2020', 'Trip_taker_work_sline_shed' ,'trip_taker_work_trct_fips_2020', 'trip_taker_work_trct_2020']].value_counts()\n",
    "\n",
    "print(\"The shape after Travel Sheds is\",  Subset_for_mapping.shape)           # 4 travel sheds: 'Origin_sline_shed', 'Destination_sline_shed','Trip_taker_home_sline_shed', 'Trip_taker_work_sline_shed'\n",
    "                                                                              #+ 1 Income bin: 'trip_taker_household_income_bin'\n",
    "\n",
    "\n",
    "print(\"The trip taker's home Travel-Shed are\",Subset_for_mapping[\"Trip_taker_home_sline_shed\"].value_counts())\n",
    "print(\"The trip taker's home Travel-Shed are\",Subset_for_mapping[\"Trip_taker_work_sline_shed\"].value_counts())\n",
    "print(\"The trip Origin Travel-Shed are\",Subset_for_mapping[\"Origin_sline_shed\"].value_counts())\n",
    "print(\"The trip Destination Travel-Shed are\",Subset_for_mapping[\"Destination_sline_shed\"].value_counts())\n",
    "\n",
    "\n",
    "#Subset_for_mapping.columns\n",
    "#Subset_for_mapping.head(10)\n",
    "\n",
    "subset_for_summary =Subset_for_mapping.copy()\n",
    "\n",
    "# Start time >> Start hour\n",
    "#subset_for_summary.rename(columns = {'trip_start_time':'trip_start_hour'}, inplace = True) \n",
    "# drop county code, trip duration, others\n",
    "subset_for_summary = subset_for_summary.drop(['origin_cty_fips_2020','origin_trct_2020','destination_trct_2020', 'destination_cty_fips_2020','trip_taker_home_trct_2020','trip_taker_home_cty_2020', 'trip_duration_minutes'],axis=1) #,'trip_duration_minutes' #'origin_cty_fips_2020','trip_taker_work_trct_2020.1',\n",
    "print(\"The shape is now\", subset_for_summary.shape)\n",
    "print(\"The columns remaining are\", subset_for_summary.columns)\n",
    "\n",
    "# Now categorizing these columns to numericals\n",
    "#Primary-mode ['private_auto', 'auto_passenger', 'public_transit','other_travel_mode', 'on_demand_auto', 'biking', 'walking'], dtype=object)\n",
    "# 1 'private_auto'\n",
    "# 2  'auto_passenger'\n",
    "# 3 'public_transit'\n",
    "# 4 'on_demand_auto'\n",
    "# 5 'biking'\n",
    "# 6 'walking'\n",
    "# 7 'other_travel_mode'\n",
    "\n",
    "#Trip purpose 'home', 'work', 'shop', 'social', 'eat', 'other_activity_type', 'maintenance', 'school', 'recreation'\n",
    "#1 'home'\n",
    "#2 'work'\n",
    "#3 'shop'\n",
    "#4 'social'\n",
    "#5 'eat'\n",
    "#6 'school'\n",
    "#7 'recreation'\n",
    "#8 'maintenance'\n",
    "#9 'other_activity_type'\n",
    "\n",
    "#TravelShed\n",
    "\n",
    "#Origin_sline_shed =           ['99', 'Tukwila', 'Auburn', 'Kent', 'Sumner', 'Lakewood', 'Puyallup', 'King Street', 'Tacoma Dome', 'South Tacoma'],dtype=object)\n",
    "#'Destination_sline_shed'=     ['99', 'Kent', 'King Street', 'Tukwila', 'Puyallup', 'Sumner',  'Tacoma Dome', 'Auburn', 'Lakewood', 'South Tacoma'], dtype=object)\n",
    "#'Trip_taker_home_sline_shed'= ['99', 'Kent', 'Tacoma Dome', 'Sumner', 'Lakewood', 'Puyallup','Auburn', 'King Street', 'Tukwila', 'South Tacoma'], dtype=object)\n",
    "#'Trip_taker_work_sline_shed'= ['99', 'King Street', 'Tukwila', 'Kent', 'Auburn', 'Tacoma Dome', 'Puyallup', 'Sumner', 'Lakewood', 'South Tacoma'], dtype=object) \n",
    "#df['trip_purpose'].unique() \n",
    "\n",
    "# Categorizing a few columns to reduce the file size\n",
    "# Creating an empty Dictionary\n",
    "Dict = {}\n",
    "Shed_list = [('Auburn',1), ('Kent',2), ('King Street',3), ('Lakewood',4), ('Puyallup',5), ('South Tacoma',6), ('Sumner',7), ('Tacoma Dome',8), ('Tukwila',9), (99,99)]\n",
    "Mode_list = [('private_auto',1), ('auto_passenger',2), ('public_transit',3), ('on_demand_auto',4), ('biking',5), ('walking',6),('other_travel_mode',7)]\n",
    "Trp_purpose_list= [('home',1), ('work',2), ('shop',3), ('social',4), ('eat',5), ('school',6), ('recreation',7), ('maintenance',8), ('other_activity_type',9)]\n",
    " \n",
    "# Creating a Dictionary # with each item as a Pair\n",
    "Shed_Dict = dict(Shed_list)\n",
    "Mode_Dict = dict(Mode_list)\n",
    "Trp_purpose_Dict= dict(Trp_purpose_list)\n",
    "#print(Dict)\n",
    "\n",
    "\n",
    "#Now Mapping\n",
    "subset_for_summary['Origin_sline_shed_numeric'] = subset_for_summary['Origin_sline_shed'].map(Shed_Dict) \n",
    "subset_for_summary['Destination_sline_shed_numeric'] = subset_for_summary['Destination_sline_shed'].map(Shed_Dict) \n",
    "subset_for_summary['Trip_taker_home_sline_shed_numeric'] = subset_for_summary['Trip_taker_home_sline_shed'].map(Shed_Dict) \n",
    "subset_for_summary['Trip_taker_work_sline_shed_numeric'] = subset_for_summary['Trip_taker_work_sline_shed'].map(Shed_Dict) \n",
    "\n",
    "subset_for_summary['Primary_Mode_numeric'] = subset_for_summary['primary_mode'].map(Mode_Dict) \n",
    "subset_for_summary['Trip_purpose_numeric'] = subset_for_summary['trip_purpose'].map(Trp_purpose_Dict)\n",
    "\n",
    "# checking the map \n",
    "subset_for_summary [['Origin_sline_shed','Origin_sline_shed_numeric','Destination_sline_shed', 'Destination_sline_shed_numeric','Trip_taker_home_sline_shed','Trip_taker_home_sline_shed_numeric','Trip_taker_work_sline_shed','Trip_taker_work_sline_shed_numeric','primary_mode','Primary_Mode_numeric','trip_purpose','Trip_purpose_numeric']]\n",
    "\n",
    "subset_for_summary[['Origin_sline_shed','Origin_sline_shed_numeric','Destination_sline_shed', 'Destination_sline_shed_numeric','Trip_taker_home_sline_shed','Trip_taker_home_sline_shed_numeric','Trip_taker_work_sline_shed','Trip_taker_work_sline_shed_numeric','primary_mode','Primary_Mode_numeric','trip_purpose','Trip_purpose_numeric']].tail(10)\n",
    "print(subset_for_summary[\"Origin_sline_shed\"].value_counts())\n",
    "\n",
    "#Now, trying to get only HBW trips & its hourly counts\n",
    "#For HBW Columns\n",
    "HBW_col = ['trip_purpose','previous_trip_purpose' ]\n",
    "\n",
    "# Concatenate values of all columns to create a new column 'concatenated'\n",
    "subset_for_summary['HBW_concat'] = subset_for_summary[HBW_col].agg('-'.join, axis=1)\n",
    "\n",
    "subset_for_summary[['trip_purpose','previous_trip_purpose','HBW_concat','trip_start_time']]\n",
    "subset_for_summary =  subset_for_summary[subset_for_summary['HBW_concat'].str.contains('work-home|home-work')] \n",
    "print(\"The HBW trips table's size is\", subset_for_summary.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file_name = os.path.basename(file_path).replace(\".csv\", \"_afterNumeric.csv\")\n",
    "file_path = os.path.join(\"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset/Final\", file_name).replace(\"\\\\\",\"/\")\n",
    "subset_for_summary.to_csv(file_path, index=False)\n",
    "\n",
    "#Since mapping is ok\n",
    "drop_col = ['trip_taker_work_cty_2020','Origin_sline_shed','Destination_sline_shed', 'Trip_taker_home_sline_shed','Trip_taker_work_sline_shed','primary_mode', 'trip_purpose']\n",
    "subset_after_numeric = subset_for_summary.drop(columns=drop_col, axis=1)\n",
    "\n",
    "#Lets rename the numeric columns & others\n",
    "\n",
    "\n",
    "subset_after_numeric = subset_after_numeric.rename(columns={'Origin_sline_shed_numeric': 'origin_travel_shed', \n",
    "                        'Destination_sline_shed_numeric': 'destination_travel_shed',\n",
    "                        'Trip_taker_home_sline_shed_numeric':'trip_taker_home_travel_shed',\n",
    "                        'Trip_taker_work_sline_shed_numeric': 'trip_taker_work_travel_shed',\n",
    "                        'Primary_Mode_numeric':'primary_mode',\n",
    "                        'Trip_purpose_numeric':'trip_purpose',\n",
    "                        'trip_start_time':'trip_start_hour'\n",
    "                        })\n",
    "\n",
    "print(\"Now the shape is\", subset_after_numeric.shape)\n",
    "print(\"The columns are\", subset_after_numeric.columns)\n",
    "\n",
    "final_subset = subset_after_numeric[['origin_trct_fips_2020','destination_trct_fips_2020','primary_mode', 'trip_purpose','previous_trip_purpose','trip_start_hour','trip_distance_miles','trip_taker_work_trct_fips_2020','trip_taker_home_trct_fips_2020','trip_taker_household_income_bin','origin_travel_shed',\n",
    "       'destination_travel_shed', 'trip_taker_home_travel_shed','trip_taker_work_travel_shed', 'HBW_concat' ]]\n",
    "\n",
    "#final_subset\n",
    "# Concatenate values of all columns to create a new column 'concatenated'\n",
    "final_subset['concatenated'] = final_subset[final_subset.columns].astype(str).agg('-'.join, axis=1)\n",
    "\n",
    "# Create a new column 'counts' by counting occurrences of each unique combination\n",
    "final_subset['trip_counts'] = final_subset.groupby('concatenated')['concatenated'].transform('count')        # I could use trip counts\n",
    "\n",
    "# Drop the 'concatenated' column if not needed anymore\n",
    "final_subset = final_subset.drop(columns=['concatenated'])\n",
    "#final_subset\n",
    "\n",
    "#hourly_counts=final_subset.groupby('trip_start_hour').size()\n",
    "\n",
    "#To minimize the file size # I should have done much earlier\n",
    "cols_toInt8 = ['primary_mode', 'trip_purpose','trip_start_hour','trip_distance_miles','origin_travel_shed','destination_travel_shed','trip_taker_home_travel_shed','trip_taker_work_travel_shed','trip_counts']\n",
    "final_subset[cols_toInt8] = final_subset[cols_toInt8].applymap(np.int8)\n",
    "\n",
    "# Writing final_subset\n",
    "#Subset_before_merged = \"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset\"\n",
    "#file_path = # already given above while reading original replica download\n",
    "file_name = os.path.basename(file_path).replace(\"_afterNumeric.csv\", \"_subset.csv\")\n",
    "file_path = os.path.join(\"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset/Final/Final_Subset\", file_name).replace(\"\\\\\",\"/\")\n",
    "final_subset.to_csv(file_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "#Sample\n",
    "file_name = os.path.basename(file_path).replace( \"_afterNumeric.csv\", \".csv\")\n",
    "path=os.path.join(\"C:/Users/USAP093587/OneDrive - WSP O365/Seattle_Transit_Project/Replica_downloads/Subset/Final/Sample_final\",\"_FinalSubset_Sample.csv\").replace(\"\\\\\",\"/\")\n",
    "final_subset.sample(100).to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking trips between travelsheds\n",
    "\n",
    "trips_within_travelshed = final_subset[(final_subset['origin_travel_shed'].isin([1,2,3,4,5,6,7,8,9])) &              #99 not in travel shed\n",
    "                                             (final_subset['destination_travel_shed'].isin([1,2,3,4,5,6,7,8,9]))]\n",
    "\n",
    "hourly_counts=trips_within_travelshed.groupby('trip_start_hour').size()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a histogram\n",
    "plt.figure(figsize=(10, 6))  # Optional: Set the figure size\n",
    "plt.bar(hourly_counts.index, hourly_counts, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Hourly Count of Trips')\n",
    "plt.grid(axis='y')  # Optional: Add grid lines\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
